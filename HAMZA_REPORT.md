# Report
The two main ideas that I explored in this work were clustering and ensembling to improve DSPy optimizers. Below, I will explain the three main tasks I worked on, the progress I made, which files contain the relevant code, and suggestions for future improvements. In the following notebooks, I walk through how to use the code that I wrote and show some results:
- ClusterFewShot.ipynb
- ExpertEnsemble.ipynb
- BestDemoEnsemble.ipynb

I wrote the following Python files, each of which implements a new DSPy optimizer:
- dspy/teleprompt/cluster_fewshot.py
- dspy/teleprompt/expert_ensemble.py
- dspy/teleprompt/best_demo_ensemble.py

# Task 1: A New Few-Shot Optimizer: ClusterFewShot
The goal here was to build a new Automatic Few-Shot Learning optimizer that constructs prompts with representative demos. By representative, I mean different kinds of demos/examples that may exist in the training set. For certain datasets that have pretty uniform examples, this may not add much benefit, but for datasets with diverse input/output examples or even various tasks, this is crucial, because the LM will benefit from seeing all types of examples in the prompt rather than some or just one. For example, given a QA dataset that consists of factual, commonsense, math, and multihop questions, we don’t want our prompt to only contain demonstrations of math problems. This feature is referred to as “Coverage” in the Demonstration Retrieval for In-Context Learning paper and is one of the three key features in their proposed approach (though they implement coverage differently).

The LabeledFewShot, BootstrapFewShot, and BootstrapFewShotWithRandomSearch optimizers randomly select a subset of training examples to include in the prompt or to bootstrap. KNNFewShot finds the nearest training examples to a given input before calling BootstrapFewShot. None of them attempt to look through all the training examples and make sure a breadth of examples are included in the final prompt - that’s where ClusterFewShot comes in.

ClusterFewShot works by embedding the training examples using a dsp vectorizer of your choice (various proprietary and open-source embedding models are available) and clustering them using the k-means clustering algorithm. The optimizer runs k-means on the embeddings for all values of k up to max_k, a parameter to the function, and uses silhouette scoring to find the optimal k value for this dataset. It then clusters the embeddings using the optimal k value and selects num_labeled_demos (another parameter to the function) examples from across the clusters. It tries to take an equal number of examples from each cluster, and at least one example is taken from every cluster, but in cases where some clusters have few examples, larger clusters may contribute more examples to the prompt. I wrote this code in dspy/teleprompt/cluster_fewshot.py.

In ClusterFewShot.ipynb, I evaluate ClusterFewShot against currently existing DSPy optimizers on three different programs: the Basic RAG and Simplified Baleen programs from the intro tutorial and a new Instruction-Following Program that I implement. The first two programs use HotPotQA and the last one uses tatsu-lab/alpaca, an instruction-tuning dataset from the HuggingFace library. I chose to use this last program + dataset because I expected that clustering would be most effective when there are very different kinds of examples in the dataset making it easier to differentiate between them. An instruction-tuning dataset consists of various kinds of instructions, so I believed it would be an appropriate choice.

I show the demonstrations that ClusterFewShot selects and compare them to those of other optimizers. I also show the cluster assignments for all training examples to inspect clustering quality, which the code stores in the predictor.cluster_assignments field of the compiled program. Overall, the clustering seems to be pretty good but definitely can be improved upon.

The evaluation results can be seen in the notebook, but in summary, ClusterFewShot outperforms LabeledFewShot and BootstrapFewShot on the Basic RAG program by 2% and 8% respectively and achieves the same performance on Simplified Baleen. Note that this is with respect to answer accuracy (exact answer match). There was no difference in retrieval score on the first program, and on the second program BootstrapFewShot did better than the rest, which is not surprising since ClusterFewShot is only focused on selecting the demos which consist of question and answer, not retrieved context. 

On the Instruction Following Program, I manually evaluated outputs on 20 test questions, since there is no single “gold” answer for the questions in this dataset. It turned out that all the programs had more or less the same outputs on the test questions, so it was not a meaningful test. This is likely because these are questions that the underlying LM (gpt-4o-mini) is able to answer without needing any demonstrations, so the effect of the bootstrapped or clustered/representative demonstrations was negligible. Given more time, I would test these programs on a dataset or model on which high-quality demonstrations are critical for performance, in order to measure the relative impact of the different optimizers.

I don’t believe ClusterFewShot’s outperforming on the Basic RAG program is significant, since on previous runs it did worse than BootstrapFewShot and the same as LabeledFewShot. However, clearly ClusterFewShot is not any worse than LabeledFewShot, and since there is still much room for improvement in its implementation, I believe it can be improved to be better than LabeledFewShot and possibly BootstrapFewShot as well. It can also be used in place of LabeledFewShot within the implementation of BootstrapFewShot to improve the latter.

A future extension to this optimizer is to implement other clustering algorithms, e.g. HDBSCAN, that the user can specify or, even better, that the optimizer can select by automatically figuring out the structure and properties of the data and determining which clustering algorithm is most suitable. Using an algorithm like HDBSCAN also avoids having to run k-means multiple times to find the optimal k value, which may be slow for large training sets.

One theoretical limitation of this approach is that unsupervised clustering is not guaranteed to separate the training examples in semantically meaningful ways. It may cluster the embeddings based on some feature that is not relevant to the various categories of examples the LM needs to see in its context. However, I show in the notebook that the current implementation of ClusterFewShot already does a decent job of separating examples with similar syntactic differences (e.g. same answer format) as well as sometimes semantic differences (e.g. clustering together questions on similar topics/tasks). This can be improved upon by using more sophisticated embedding and clustering techniques (e.g., dimensionality reduction).

# Task 2: Routing to the Expert: Clustering and Ensembling Fine-tuned LMs
Task 2 seeks to extend the idea behind task 1 to the arena of fine-tuning and, in addition, create an ensemble of fine-tuned expert LMs that can be used in the framework introduced in the MoRE paper. In other words, instead of finetuning a single model on the training set, we can partition the training set into different clusters, finetune the underlying LM on each cluster, and route inputs at inference time to the appropriate expert LM for maximal accuracy. 

In dspy/teleprompt/expert_ensemble.py, I develop a new optimizer called ExpertEnsemble that works as previously described. Like ClusterFewShot, it currently uses k-means clustering and runs it multiple times to find the optimal k value using silhouette scoring, but can be extended to use other clustering algorithms. I reuse code from finetune.py that fine-tunes a HuggingFace model (t5-small) on given input-output pairs, and I fine-tune one model per cluster on the examples in that cluster. The k-means model (i.e. the clusters) and the fine-tuned LMs are all saved to the program’s internal state to be used later. At inference time, the code compares an input question to the cluster centroids that were computed at compilation time, and assigns it to the cluster with the nearest centroid. The corresponding fine-tuned LM is then called with this question as the prompt and its response is returned.

One important fix that’s needed in my code is to update the models in each predictor to their fine-tuned versions, without throwing out the rest of the program. I wasn’t able to figure out how to do that at inference time, so the current implementation just routes the input question to the relevant expert model without factoring in the signatures and modules of the program.

In ExpertEnsemble.ipynb, I demonstrate how to use ExpertEnsemble (same usage pattern as other optimizers) and show that it compiles and runs properly. There is logging during compilation that says which cluster is currently being fine-tuned, and after compilation you can inspect the clusters and see the different fine-tuned LM experts. 

In the same notebook, I also attempted to evaluate ExpertEnsemble against various optimizers and fine-tuned models on a simple GSM8K program. BoostrapFinetune fine-tunes a single LM using the same setup as my optimizer (since I borrowed the fine-tuning code from BootstrapFinetune to implement ExpertEnsemble) and thus would serve as a good baseline. I was getting errors trying to evaluate and run a program compiled by BootstrapFinetune however, so instead I tried to fine-tune an LM outside of an optimizer. 

I tried several LMs (gpt-2, t5-small, t5-base, t5-large, flan-t5-small) and several implementations (HF Transformers Trainer, PyTorch w/o HF, dspy.modules.finetuning.finetune_hf), however they all seemed to really struggle on the GSM8K task. Despite my best efforts to train with different hyperparameters (different number of epochs, batch size, training set size, and input formatting), models would often respond to questions by just repeating parts of the question or rambling incoherently. Out of several tests I only saw one (most likely coincidental) correct answer. Thus, I unfortunately wasn’t able to meaningfully evaluate my optimizer with the time I had. 

Given more time, I would try to debug DSPy’s BootstrapFinetune, and then try to figure out what’s causing the very low model performance. It’s possible that these models require much more data and much longer training than in my tests to do well on GSM8K (though flan-t5 was finetuned on GSM8K so I was surprised that it didn’t do well right out of the box). After making these fixes I would be able to properly evaluate ExpertEnsemble against a single fine-tuned model and other DSPy optimizers.

Note that I do not fully implement the framework from the MoRE paper, which entails running all the expert models on the test question and training a classifier to take in all the predictions, their confidence, and their agreement and select a final answer (or choose to abstain). Instead, I chose to simply route to the relevant expert. An avenue for future work would be to extend this code to implement the entire framework, which would likely improve performance since some questions may not closely match any expert’s domain and would benefit from the collaborative approach.

Another avenue for future work would be to improve the routing mechanism, so that instead of relying on distance to the cluster centroids to assign an input at inference time to an expert LM, we train a model to learn how to route inputs appropriately, perhaps using reward models like in the Routing to the Expert paper.

# Task 3: LLM-as-Judge for Optimizer Selection
Finally, I had an idea related to ensembling optimizers themselves. As more and more optimizers are added to DSPy, a user will have many options for how to compile their program without a systematic way of choosing the best one. In the spirit of automating and systematizing LM programming, it would make sense for DSPy to have functionality that automatically selects the best optimizer to use, based on the features of the program being compiled and/or the input examples. Currently, DSPy has an Ensemble optimizer but it returns all the outputs of multiple programs provided by the user, rather than combining the answers or selecting the best one.

A step towards this goal is to be able to determine the best optimizer within just the Automatic Few-Shot Learning class of optimizers currently available in DSPy. I thought that one way to do this is to have an LM judge the demos selected by the various optimizers and choose whichever program has the best demos. The LM can be instructed (or taught) to judge the demos using any criteria we choose, such as the three linguistically-grounded criteria from the Demonstration Retrieval for In-Context Learning paper: difficulty, coverage, and semantic shift. This is the approach I take in my code; I instruct the LM to choose using these three criteria and to provide its reasoning for explainability. I save which program is chosen and the LM’s justification to the optimizer’s internal state so the user can inspect why the LM chose the program that it did.

I implemented this functionality inside a new optimizer, which I call BestDemoEnsemble, in the best_demo_ensemble.py file and demonstrated its usage on a simple QA program in BestDemoEnsemble.ipynb. You pass in the compiled programs you want it to choose between and an LM of your choice to be the judge - that’s all! With that being said, the current implementation is quite fragile since it assumes the LM will return a single number representing which program to select, and if it decides to ignore its instructions and return something else, then the code will break. We can fix this by using an LM output constraint technique from the literature.

A couple improvements that can be made: 
1. Have the user only pass in the uncompiled base program, and we compile it using various optimizers before choosing between them. This way the client doesn’t have to compile the same program multiple times to use this optimizer. 
2. Fine-tune or provide in-context examples for the LM to learn this task, as we are  currently using the LM in a zero-shot manner.

More broadly, there are many possibilities to explore in this space of ensembling and automatic selection of optimizers. At one end of the spectrum, without compiling multiple times, we can automatically select an optimizer by:
- Using a rule-based system (automating whatever rules DSPy currently tells users to apply when choosing an optimizer)
- Learning a classifier to select the best optimizer given a program
  
At the other end, we can form an ensemble of compiled programs (ie the base program compiled with different optimizers) and:
- Brute force evaluate them all on a test set, choosing the one that does best (or the one that does best on the training set + metric during compilation)
- Run all of them on an input and learn a classifier to select the final answer (MoRE approach)
- Learn a router that selects the appropriate program for each given input (Routing to the Expert)
- Choose the best one by analyzing the prompt/weight updates of each (the approach I used falls into this category)
